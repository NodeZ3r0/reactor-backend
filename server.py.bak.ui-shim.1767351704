import os
import asyncpg
from fastapi import FastAPI, HTTPException, Request, Depends
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import JSONResponse
from pydantic import BaseModel
from typing import List, Optional
from dotenv import load_dotenv
import httpx
import uuid

load_dotenv()

# Database wrapper
class Database:
    def __init__(self):
        self.pool = None

    async def init_pool(self):
        db_url = os.getenv("DATABASE_URL")
        if not db_url:
            raise RuntimeError("DATABASE_URL not set")
        self.pool = await asyncpg.create_pool(db_url, min_size=2, max_size=10)
        print(f"✓ Database pool created")

# Repo base path
REPOS_BASE_PATH = os.getenv("REPOS_BASE_PATH") or "/opt/mcp-agent/repos"
os.makedirs(REPOS_BASE_PATH, exist_ok=True)

# API key auth
MCP_API_KEY = os.getenv("MCP_API_KEY")
if not MCP_API_KEY:
    raise RuntimeError("MCP_API_KEY is not set in .env")

# Ollama config
OLLAMA_BASE_URL = os.getenv("OLLAMA_BASE_URL", "http://10.0.0.3:11434")

# Model fallback chain
MODEL_FALLBACK_CHAIN = [
    "hermes3:70b",
    "qwen2.5-coder:32b",
    "qwen2.5-coder:14b",
    "qwen2.5-coder:7b",
]

# Version
CODING_AGENT_VERSION = "2.12.2025-1"

db = Database()

app = FastAPI(
    title="NodeZ3r0 Coding Agent MCP Server",
    version=CODING_AGENT_VERSION,
)

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Global error handler
@app.exception_handler(Exception)
async def global_exception_handler(request: Request, exc: Exception):
    return JSONResponse(
        status_code=500,
        content={"detail": f"{type(exc).__name__}: {str(exc)}"},
    )

# API key auth
async def verify_api_key(request: Request):
    header_key = request.headers.get("X-API-Key")
    if not header_key or header_key != MCP_API_KEY:
        raise HTTPException(status_code=401, detail="Invalid or missing API key")
    return {"username": "nodez3r0"}

# Pydantic models
class ProjectRequest(BaseModel):
    name: str
    repo_name: str

class Project(BaseModel):
    project_id: str
    name: str
    repo_name: str
    created_at: Optional[str] = None

class ProjectsResponse(BaseModel):
    projects: List[Project]

class CreateProjectResponse(BaseModel):
    status: str
    project_id: str

class GenerateRequest(BaseModel):
    project_id: str
    prompt: str
    model: Optional[str] = "hermes3:70b"
    file_path: Optional[str] = None

class GenerateResponse(BaseModel):
    status: str
    content: str
    model_used: str
    file_path: Optional[str] = None

# Helper to get DB connection
async def get_conn():
    if not db.pool:
        raise RuntimeError("Database pool not initialized")
    return await db.pool.acquire()

# Ollama helpers
async def call_ollama(prompt: str, model: str, max_tokens: int = 4096) -> Optional[str]:
    """Call Ollama API via Nebula mesh to Rig"""
    try:
        async with httpx.AsyncClient(timeout=120.0) as client:
            response = await client.post(
                f"{OLLAMA_BASE_URL}/api/generate",
                json={
                    "model": model,
                    "prompt": prompt,
                    "stream": False,
                    "options": {
                        "num_predict": max_tokens,
                        "temperature": 0.7,
                    }
                }
            )
            if response.status_code == 200:
                data = response.json()
                return data.get("response", "")
            else:
                print(f"Ollama error ({model}): {response.status_code}")
                return None
    except Exception as e:
        print(f"Ollama exception ({model}): {e}")
        return None

async def generate_with_fallback(prompt: str, preferred_model: str = None) -> tuple[str, str]:
    """Try to generate code with fallback chain"""
    models = [preferred_model] + MODEL_FALLBACK_CHAIN if preferred_model else MODEL_FALLBACK_CHAIN
    models = list(dict.fromkeys([m for m in models if m]))
    
    for model in models:
        print(f"Trying model: {model}")
        content = await call_ollama(prompt, model, max_tokens=8192)
        if content and len(content) > 50:
            return content, model
    
    raise HTTPException(
        status_code=503,
        detail="All AI models failed. Check Ollama on Rig."
    )

# Endpoints
@app.get("/api/health", response_model=dict)
async def health():
    return {"status": "ok", "version": CODING_AGENT_VERSION}

@app.get("/api/version", response_model=dict)
async def version():
    return {"version": CODING_AGENT_VERSION}

@app.post("/api/projects", response_model=CreateProjectResponse)
async def create_project(
    req: ProjectRequest,
    _user=Depends(verify_api_key),
):
    if not req.repo_name or not all(c.isalnum() or c in "-_" for c in req.repo_name):
        raise HTTPException(status_code=400, detail="Invalid repo_name")

    repo_path = os.path.join(REPOS_BASE_PATH, req.repo_name)
    os.makedirs(repo_path, exist_ok=True)

    sql = """
        INSERT INTO agent_projects (name, slug, repo_name, repo_path, created_by)
        VALUES ($1, lower(replace($1, ' ', '-')), $2, $3, $4)
        RETURNING project_id
    """

    conn = await get_conn()
    try:
        row = await conn.fetchrow(sql, req.name, req.repo_name, repo_path, "nodez3r0")
    finally:
        await db.pool.release(conn)

    project_id = str(row["project_id"])
    return CreateProjectResponse(status="success", project_id=project_id)

@app.get("/api/projects", response_model=ProjectsResponse)
async def list_projects(_user=Depends(verify_api_key)):
    sql = """
        SELECT project_id, name, repo_name, created_at
        FROM agent_projects
        ORDER BY created_at DESC
    """
    conn = await get_conn()
    try:
        rows = await conn.fetch(sql)
    finally:
        await db.pool.release(conn)

    projects: List[Project] = []
    for r in rows:
        projects.append(
            Project(
                project_id=str(r["project_id"]),
                name=r["name"],
                repo_name=r["repo_name"],
                created_at=r["created_at"].isoformat() if r["created_at"] else None,
            )
        )
    return ProjectsResponse(projects=projects)

@app.post("/api/generate", response_model=GenerateResponse)
async def generate_code(
    req: GenerateRequest,
    _user=Depends(verify_api_key),
):
    """Generate code using AI models on Rig via Nebula"""
    
    conn = await get_conn()
    try:
        row = await conn.fetchrow(
            "SELECT repo_path, name FROM agent_projects WHERE project_id = $1",
            uuid.UUID(req.project_id)
        )
    finally:
        await db.pool.release(conn)
    
    if not row:
        raise HTTPException(status_code=404, detail="Project not found")
    
    repo_path = row["repo_path"]
    project_name = row["name"]
    
    enhanced_prompt = f"""Project: {project_name}
Repository: {repo_path}

Task: {req.prompt}

Instructions:
- Generate production-quality code
- Include proper error handling
- Add comments for complex logic
- Follow best practices
- Do not include explanatory text, only code

Code:
"""
    
    content, model_used = await generate_with_fallback(enhanced_prompt, req.model)
    
    saved_file_path = None
    if req.file_path:
        full_path = os.path.join(repo_path, req.file_path)
        os.makedirs(os.path.dirname(full_path), exist_ok=True)
        
        with open(full_path, "w") as f:
            f.write(content)
        
        saved_file_path = req.file_path
        print(f"✓ Saved generated code to: {full_path}")
    
    return GenerateResponse(
        status="success",
        content=content,
        model_used=model_used,
        file_path=saved_file_path
    )

@app.get("/api/models", response_model=dict)
async def list_models(_user=Depends(verify_api_key)):
    """List available Ollama models on Rig"""
    try:
        async with httpx.AsyncClient(timeout=10.0) as client:
            response = await client.get(f"{OLLAMA_BASE_URL}/api/tags")
            if response.status_code == 200:
                data = response.json()
                models = [m["name"] for m in data.get("models", [])]
                return {
                    "status": "ok",
                    "models": models,
                    "ollama_url": OLLAMA_BASE_URL
                }
            else:
                return {"status": "error", "detail": "Ollama not responding"}
    except Exception as e:
        return {"status": "error", "detail": str(e)}

@app.on_event("startup")
async def startup():
    await db.init_pool()
    print("✓ MCP Server started (VPS mode with Nebula to Rig)")

@app.on_event("shutdown")
async def shutdown():
    if db.pool:
        await db.pool.close()
    print("✓ MCP Server shut down")

if __name__ == "__main__":
    import uvicorn
    port = int(os.getenv("MCP_PORT", "7003"))
    uvicorn.run(app, host="0.0.0.0", port=port)
